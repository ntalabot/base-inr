{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7edcbc2e",
   "metadata": {},
   "source": [
    "Test the `src` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import torch\n",
    "import trimesh\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import src\n",
    "from src import workspace as ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02303ca",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00000d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed\n",
    "\n",
    "seed = 0\n",
    "expdir = \"../experiments/src_test/\"\n",
    "set_seed(seed)\n",
    "ws.build_experiment_dir(expdir)\n",
    "specs = ws.load_specs(expdir)\n",
    "\n",
    "print(f\"Running experiment in {expdir}\")\n",
    "print(f\"Seeds initialized to {seed}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f199e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14620e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import SdfDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "n_samples = 8192\n",
    "\n",
    "with open(specs[\"TrainSplit\"]) as f:\n",
    "    instances = json.load(f)\n",
    "\n",
    "dataset = SdfDataset(specs[\"DataSource\"], instances, n_samples, \n",
    "                     specs[\"SamplesDir\"], specs[\"SamplesFile\"])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "len_dataset = len(dataset)\n",
    "\n",
    "print(f\"{len_dataset} shapes in training dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from matplotlib import cm\n",
    "\n",
    "idx = 0\n",
    "filename = dataset.filenames[idx]\n",
    "idx, points, sdf = dataset[0]\n",
    "print(f\"{len(points)} points for {filename} shape.\")\n",
    "\n",
    "N = 1000\n",
    "cmap = colormaps['bwr']\n",
    "c = np.clip(sdf[:N,0], -0.1, 0.1)\n",
    "vmax = np.abs(c).max()\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p = ax.scatter(points[:N,0], points[:N,1], points[:N,2], c=c, cmap=cmap, vmin=-vmax, vmax=vmax)\n",
    "fig.colorbar(p)\n",
    "ax.set_title(\"Recon. samples\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd0d72",
   "metadata": {},
   "source": [
    "# Model and latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import get_model, get_latents, features\n",
    "\n",
    "latent_dim = 128\n",
    "model = get_model(\n",
    "    \"LatentModulatedDeepSDF\",\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=256,\n",
    "    n_layers=6,\n",
    "    dropout=0.,\n",
    "    activation=\"relu\",\n",
    "    features = None\n",
    ").cuda()\n",
    "\n",
    "latents = get_latents(len(dataset), latent_dim, None)\n",
    "\n",
    "print(f\"Model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")\n",
    "print(f\"{latents.num_embeddings} latent vectors of size {latents.embedding_dim}.\")\n",
    "\n",
    "# Initialize history\n",
    "history = {'epoch': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2b28b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49501f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.loss import get_loss_recon\n",
    "from src.optimizer import get_optimizer, get_scheduler\n",
    "from src.utils import clamp_sdf\n",
    "\n",
    "n_epochs = 20\n",
    "clampD = 0.1\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_recon = get_loss_recon(\"L1-Hard\", reduction='none')\n",
    "latent_reg = 1e-4\n",
    "\n",
    "optimizer = get_optimizer([model, latents], type=\"adam\", lrs=[0.0005, 0.001])\n",
    "scheduler = get_scheduler(optimizer, Type=\"Constant\")\n",
    "\n",
    "# Training\n",
    "for key in ['loss', 'loss_reg', 'lr', 'lr_lat', 'lat_norm']:\n",
    "    if key not in history:\n",
    "        history[key] = []\n",
    "model.train()\n",
    "for epoch in range(history['epoch']+1, n_epochs+1):\n",
    "    time_epoch = time.time()\n",
    "    running_losses = {'loss': 0., 'loss_reg': 0.}\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (indices, xyz, sdf_gt) in enumerate(dataloader):\n",
    "        xyz = xyz.cuda()  # BxNx3\n",
    "        sdf_gt = sdf_gt.cuda()  # BxNx1\n",
    "        indices = indices.cuda().unsqueeze(-1).repeat(1, xyz.shape[1])  # BxN\n",
    "        batch_latents = latents(indices)  # BxNxL\n",
    "\n",
    "        inputs = torch.cat([batch_latents, xyz], dim=-1)  # BxNx(L+3)\n",
    "        sdf_pred = model(inputs)\n",
    "        if clampD is not None and clampD > 0.:\n",
    "            sdf_pred = clamp_sdf(sdf_pred, clampD, ref=sdf_gt)\n",
    "            sdf_gt = clamp_sdf(sdf_gt, clampD)\n",
    "\n",
    "        loss = loss_recon(sdf_pred, sdf_gt).mean()\n",
    "        running_losses['loss'] += loss.item() * batch_size\n",
    "        # Latent regularization\n",
    "        if latent_reg is not None and latent_reg > 0.:\n",
    "            loss_reg = min(1, epoch / 100) * batch_latents[:,0,:].square().sum()\n",
    "            loss = loss + latent_reg * loss_reg\n",
    "            running_losses['loss_reg'] += loss_reg.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    history['epoch'] += 1\n",
    "    history['loss'].append(running_losses['loss'] / len_dataset)\n",
    "    history['loss_reg'].append(running_losses['loss_reg'] / len_dataset)\n",
    "    history[\"lr\"].append(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "    history[\"lr_lat\"].append(optimizer.state_dict()[\"param_groups\"][1][\"lr\"])\n",
    "    lat_norms = torch.norm(latents.weight.data.detach(), dim=1).cpu()\n",
    "    history[\"lat_norm\"].append(lat_norms.mean())\n",
    "\n",
    "    # Apply lr-schedule\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}: loss={loss.item():.6f} - loss_reg={loss_reg.item():.6f}\" + \\\n",
    "          f\" ({time.time() - time_epoch:.0f}s/epoch)\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12,3))\n",
    "axs[0].plot(history['loss'])\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].plot(history['loss_reg'])\n",
    "axs[1].set_title(\"Reg. loss\")\n",
    "axs[2].plot(history['lr'])\n",
    "axs[2].plot(history['lr_lat'])\n",
    "axs[2].legend(['lr', 'lr_lat'])\n",
    "axs[2].set_title(\"LRs\")\n",
    "axs[3].plot(history['lat_norm'])\n",
    "axs[3].set_title(\"Lat. norm\")\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61573bc",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae070e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.utils import sample_latents as _sample_latents\n",
    "\n",
    "_pca = PCA(whiten=True).fit(latents.weight.detach().cpu().numpy())\n",
    "def sample_latents(n=1, expvar=None):\n",
    "    \"\"\"PCA sampling of latent(s) from training distribution.\"\"\"\n",
    "    return _sample_latents(latents, n_samples=n, expvar=expvar, pca=_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04da2fc",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDF\n",
    "from src.utils import make_grid2d\n",
    "from src.mesh import compute_sdf\n",
    "\n",
    "idx = 0\n",
    "clamp = True\n",
    "\n",
    "clamp &= clampD is not None and clampD > 0.\n",
    "latent = latents(torch.tensor([idx]).cuda()) if idx is not None else sample_latents()\n",
    "print(f\"Clamping at {clampD}.\" if clamp else \"No clamping.\")\n",
    "\n",
    "model.eval()\n",
    "cmap = colormaps['bwr']\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 3.5))\n",
    "for i, (ax, ax_name) in enumerate(zip(axs.flatten(), ['x', 'y', 'z'])):\n",
    "    xyz = make_grid2d([[-1, -1], [1, 1]], 512, i, 0.)\n",
    "    with torch.no_grad():\n",
    "        sdf = compute_sdf(model, latent, xyz.cuda()).squeeze().detach().cpu()\n",
    "\n",
    "    vmax = min(vmax, clampD) if clamp else sdf.abs().max()\n",
    "    ax.set_title(f\"SDF at {ax_name}=0.\")\n",
    "    im = ax.imshow(sdf.T.flip(0), cmap=cmap, vmin=-vmax, vmax=vmax, extent=[-1,1,-1,1])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5205ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesh\n",
    "from src.mesh import create_mesh\n",
    "\n",
    "idx = 0\n",
    "latent = latents(torch.tensor([idx]).cuda()) if idx is not None else sample_latents()\n",
    "mesh = create_mesh(model, latent, 256, 32**3, verbose=True)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f181e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering\n",
    "from src import visualization as viz\n",
    "\n",
    "image = viz.render_mesh(mesh)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd792b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "from src.reconstruct import reconstruct\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with open(specs[\"TestSplit\"]) as f:\n",
    "    instances_t = json.load(f)\n",
    "instance = instances_t[idx]\n",
    "print(f\"Reconstructing test shape {idx} ({instance})\")\n",
    "\n",
    "filename = os.path.join(specs[\"DataSource\"], specs[\"SamplesDir\"], instance, specs[\"SamplesFile\"])\n",
    "npz = np.load(filename)\n",
    "\n",
    "err, latent = reconstruct(model, npz, 400, 8000, 5e-3, loss_recon, latent_reg, clampD, None, latent_dim, verbose=True)\n",
    "print(f\"Final error: {err:.6f}.\")\n",
    "print(f\"Latent: norm={latent.norm():.4f} - std={latent.std():.4f}\")\n",
    "test_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=True, verbose=True)\n",
    "\n",
    "filename = os.path.join(specs[\"DataSource\"], \"meshes\", instance+\".obj\")\n",
    "gt_mesh = trimesh.load(filename)\n",
    "\n",
    "images = viz.render_meshes([gt_mesh, test_mesh])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(images[0]); axs[0].set_title(\"GT\")\n",
    "axs[1].imshow(images[1]); axs[1].set_title(\"Reconstruction\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from src.metric import chamfer_distance\n",
    "\n",
    "chamfer_samples = 30_000\n",
    "\n",
    "gt_samples = gt_mesh.sample(chamfer_samples)\n",
    "recon_samples = test_mesh.sample(chamfer_samples)\n",
    "chamfer_val = chamfer_distance(gt_samples, recon_samples)\n",
    "print(f\"Chamfer-distance = {chamfer_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914756c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f22d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Stop here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e5e87",
   "metadata": {},
   "source": [
    "# Test SDF query speed\n",
    "Test speed of queriyng SDF (+grad) with a formula, IGL, and a network (DeepSDF like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d118ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import igl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e925ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sphere: formula vs. IGL\n",
    "def sphere_SDF_grad(xyz):\n",
    "    norms = np.linalg.norm(xyz, axis=-1, keepdims=True)\n",
    "    return norms - 0.5, xyz / norms\n",
    "\n",
    "\n",
    "sphere_mesh = trimesh.creation.uv_sphere(0.5, [32, 32])\n",
    "print(len(sphere_mesh.faces))\n",
    "def igl_sphere(xyz):\n",
    "    sdf, _, _, grads = igl.signed_distance(xyz, sphere_mesh.vertices, sphere_mesh.faces, return_normals=True)\n",
    "    return sdf, grads\n",
    "\n",
    "# 800 time-steps, 2048 particles\n",
    "xyz = np.random.rand(800, 2048, 3) * 2 - 1\n",
    "\n",
    "formula_times = []\n",
    "igl_times = []\n",
    "for t in range(len(xyz)):\n",
    "    start = time.perf_counter()\n",
    "    sdf, grads = sphere_SDF_grad(xyz[t])\n",
    "    formula_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    sdf, grads = igl_sphere(xyz[t])\n",
    "    igl_times.append(time.perf_counter() - start)\n",
    "sphere_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0dace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(formula_times), np.mean(formula_times), np.std(formula_times))\n",
    "print(np.sum(igl_times), np.mean(igl_times), np.std(igl_times))\n",
    "plt.hist(formula_times, bins=20);\n",
    "plt.hist(igl_times, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGL on chairs\n",
    "\n",
    "chair = trimesh.load(\n",
    "    \"/cvlabsrc1/cvlab/datasets_talabot/ShapeNet/ShapeNetV2_raw/Chair/20ae4b27e86521a32efc7fb40a53aaac/model.obj\"\n",
    ")\n",
    "chair.show()\n",
    "\n",
    "print(len(chair.faces))\n",
    "def igl_chair(xyz):\n",
    "    sdf, _, _, grads = igl.signed_distance(xyz, chair.vertices, chair.faces, return_normals=True)\n",
    "    return sdf, grads\n",
    "\n",
    "# 800 time-steps, 2048 particles\n",
    "xyz = np.random.rand(800, 2048, 3) * 2 - 1\n",
    "\n",
    "igl_times2 = []\n",
    "for t in range(len(xyz)):\n",
    "    start = time.perf_counter()\n",
    "    sdf, grads = igl_chair(xyz[t])\n",
    "    igl_times2.append(time.perf_counter() - start)\n",
    "print(np.sum(igl_times2), np.mean(igl_times2), np.std(igl_times))\n",
    "plt.hist(igl_times2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network (untrained)\n",
    "mynet = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 512), torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 1)\n",
    ").cuda()\n",
    "\n",
    "xyz = (torch.rand((800, 2048, 3)) * 2 - 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f16d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_times = []\n",
    "for t in range(len(xyz)):\n",
    "    start = time.perf_counter()\n",
    "    xyz_ = xyz[t].requires_grad_()\n",
    "    sdf = mynet(xyz_)\n",
    "    grads, = torch.autograd.grad(sdf.sum(), xyz_)\n",
    "    _ = f\"{sdf[0]}\"\n",
    "    dnn_times.append(time.perf_counter() - start)\n",
    "print(np.sum(dnn_times), np.mean(dnn_times), np.std(dnn_times))\n",
    "plt.hist(dnn_times, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530005f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile() as prof:\n",
    "    for t in range(len(xyz)):\n",
    "        xyz_ = xyz[t].requires_grad_()\n",
    "        sdf = mynet(xyz_)\n",
    "        grads, = torch.autograd.grad(sdf.sum(), xyz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: some columns were removed for brevity\n",
    "print(prof.total_average())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4281e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.003*800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbec55",
   "metadata": {},
   "source": [
    "## Copy dataset\n",
    "Copy a minimal version of the dataset for student projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02032971",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = \"/cvlabsrc1/cvlab/datasets_talabot/shapenet_disn/1_normalized/chairs/\"\n",
    "\n",
    "with open(os.path.join(datasource, \"splits/chairs_train1210_benoit.json\")) as f:\n",
    "    instances = json.load(f)\n",
    "    \n",
    "with open(os.path.join(datasource, \"splits/chairs_test113_benoit.json\")) as f:\n",
    "    instances_t = json.load(f)\n",
    "\n",
    "full_len = len(instances) + len(instances_t)\n",
    "print(f\"{full_len} chairs to copy.\")\n",
    "\n",
    "os.makedirs(os.path.join(datasource, f\"chairs_{full_len}\"))\n",
    "os.makedirs(os.path.join(datasource, f\"chairs_{full_len}/meshes\"))\n",
    "os.makedirs(os.path.join(datasource, f\"chairs_{full_len}/samples\"))\n",
    "os.makedirs(os.path.join(datasource, f\"chairs_{full_len}/splits\"))\n",
    "\n",
    "for instance in instances + instances_t:\n",
    "    shutil.copyfile(os.path.join(datasource, f\"meshes/{instance}.obj\"),\n",
    "                    os.path.join(datasource, f\"chairs_{full_len}/meshes/{instance}.obj\"))\n",
    "    \n",
    "    os.makedirs(os.path.join(datasource, f\"chairs_{full_len}/samples/{instance}\"))\n",
    "    shutil.copyfile(os.path.join(datasource, f\"samples/{instance}/deepsdf.npz\"),\n",
    "                    os.path.join(datasource, f\"chairs_{full_len}/samples/{instance}/deepsdf.npz\"))\n",
    "\n",
    "with open(os.path.join(datasource, f\"chairs_{full_len}/splits/chairs_train{len(instances)}.json\"), 'w') as f:\n",
    "    json.dump(instances, f, indent=2)\n",
    "    \n",
    "with open(os.path.join(datasource, f\"chairs_{full_len}/splits/chairs_test{len(instances_t)}.json\"), 'w') as f:\n",
    "    json.dump(instances_t, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b983f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
