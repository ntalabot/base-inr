{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7edcbc2e",
   "metadata": {},
   "source": [
    "Test the `src` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import torch\n",
    "import trimesh\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import src\n",
    "from src import workspace as ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02303ca",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00000d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed\n",
    "\n",
    "seed = 0\n",
    "expdir = \"../experiments/src_test/\"\n",
    "set_seed(seed)\n",
    "ws.build_experiment_dir(expdir)\n",
    "specs = ws.load_specs(expdir)\n",
    "\n",
    "print(f\"Running experiment in {expdir}\")\n",
    "print(f\"Seeds initialized to {seed}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f199e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14620e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import SdfDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "n_samples = 8192\n",
    "\n",
    "with open(specs[\"TrainSplit\"]) as f:\n",
    "    instances = json.load(f)\n",
    "\n",
    "dataset = SdfDataset(specs[\"DataSource\"], instances, n_samples, \n",
    "                     specs[\"SamplesDir\"], specs[\"SamplesFile\"])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "len_dataset = len(dataset)\n",
    "\n",
    "print(f\"{len_dataset} shapes in training dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from matplotlib import cm\n",
    "\n",
    "idx = 0\n",
    "filename = dataset.filenames[idx]\n",
    "idx, points, sdf = dataset[0]\n",
    "print(f\"{len(points)} points for {filename} shape.\")\n",
    "\n",
    "N = 1000\n",
    "_indices = np.random.permutation(len(points))[:N]\n",
    "points, sdf = points[_indices], sdf[_indices]\n",
    "cmap = colormaps['bwr']\n",
    "c = np.clip(sdf[:,0], -0.1, 0.1)\n",
    "vmax = np.abs(c).max()\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p = ax.scatter(points[:,0], points[:,1], points[:,2], c=c, cmap=cmap, vmin=-vmax, vmax=vmax)\n",
    "fig.colorbar(p)\n",
    "ax.set_title(\"Recon. samples\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd0d72",
   "metadata": {},
   "source": [
    "# Model and latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import get_model, get_latents, features\n",
    "\n",
    "latent_dim = specs[\"LatentDim\"]\n",
    "model = get_model(\n",
    "    specs[\"Network\"],\n",
    "    **specs[\"NetworkSpecs\"]\n",
    ").cuda()\n",
    "\n",
    "latents = get_latents(len(dataset), latent_dim, None)\n",
    "\n",
    "print(f\"Model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")\n",
    "print(f\"{latents.num_embeddings} latent vectors of size {latents.embedding_dim}.\")\n",
    "\n",
    "# Initialize history\n",
    "history = {'epoch': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2b28b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49501f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.loss import get_loss_recon\n",
    "from src.optimizer import get_optimizer, get_scheduler\n",
    "from src.utils import clamp_sdf\n",
    "\n",
    "n_epochs = 20\n",
    "clampD = specs[\"ClampingDistance\"]\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_recon = get_loss_recon(\"L1-Hard\", reduction='none')\n",
    "latent_reg = specs[\"LatentRegLambda\"]\n",
    "\n",
    "optimizer = get_optimizer([model, latents], type=\"adam\", lrs=[0.0005, 0.001])\n",
    "scheduler = get_scheduler(optimizer, Type=\"Constant\")\n",
    "\n",
    "# Training\n",
    "for key in ['loss', 'loss_reg', 'lr', 'lr_lat', 'lat_norm']:\n",
    "    if key not in history:\n",
    "        history[key] = []\n",
    "model.train()\n",
    "for epoch in range(history['epoch']+1, n_epochs+1):\n",
    "    time_epoch = time.time()\n",
    "    running_losses = {'loss': 0., 'loss_reg': 0.}\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (indices, xyz, sdf_gt) in enumerate(dataloader):\n",
    "        xyz = xyz.cuda()  # BxNx3\n",
    "        sdf_gt = sdf_gt.cuda()  # BxNx1\n",
    "        indices = indices.cuda().unsqueeze(-1)  # Bx1\n",
    "        batch_latents = latents(indices)  # Bx1xL\n",
    "\n",
    "        sdf_pred = model(batch_latents, xyz)\n",
    "        if clampD is not None and clampD > 0.:\n",
    "            sdf_pred = clamp_sdf(sdf_pred, clampD, ref=sdf_gt)\n",
    "            sdf_gt = clamp_sdf(sdf_gt, clampD)\n",
    "\n",
    "        loss = loss_recon(sdf_pred, sdf_gt).mean()\n",
    "        running_losses['loss'] += loss.item() * batch_size\n",
    "        # Latent regularization\n",
    "        if latent_reg is not None and latent_reg > 0.:\n",
    "            loss_reg = min(1, epoch / 100) * batch_latents[:,0,:].square().sum()\n",
    "            loss = loss + latent_reg * loss_reg\n",
    "            running_losses['loss_reg'] += loss_reg.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    history['epoch'] += 1\n",
    "    history['loss'].append(running_losses['loss'] / len_dataset)\n",
    "    history['loss_reg'].append(running_losses['loss_reg'] / len_dataset)\n",
    "    history[\"lr\"].append(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "    history[\"lr_lat\"].append(optimizer.state_dict()[\"param_groups\"][1][\"lr\"])\n",
    "    lat_norms = torch.norm(latents.weight.data.detach(), dim=1).cpu()\n",
    "    history[\"lat_norm\"].append(lat_norms.mean())\n",
    "\n",
    "    # Apply lr-schedule\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}: loss={loss.item():.6f} - loss_reg={loss_reg.item():.6f}\" + \\\n",
    "          f\" ({time.time() - time_epoch:.0f}s/epoch)\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12,3))\n",
    "axs[0].plot(history['loss'])\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].plot(history['loss_reg'])\n",
    "axs[1].set_title(\"Reg. loss\")\n",
    "axs[2].plot(history['lr'])\n",
    "axs[2].plot(history['lr_lat'])\n",
    "axs[2].legend(['lr', 'lr_lat'])\n",
    "axs[2].set_title(\"LRs\")\n",
    "axs[3].plot(history['lat_norm'])\n",
    "axs[3].set_title(\"Lat. norm\")\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import derivative as deriv\n",
    "\n",
    "lat = latents(torch.tensor([0]).cuda())\n",
    "xyz = torch.randn(10, 3).cuda()\n",
    "\n",
    "model.eval()\n",
    "out1 = deriv.get_laplacian(model, lat, xyz.requires_grad_())\n",
    "out2 = deriv.get_laplacian_fd(model, lat, xyz.requires_grad_(), 1e-2)\n",
    "out1 - out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61573bc",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae070e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.utils import sample_latents as _sample_latents\n",
    "\n",
    "_pca = PCA(whiten=True).fit(latents.weight.detach().cpu().numpy())\n",
    "def sample_latents(n=1, expvar=None):\n",
    "    \"\"\"PCA sampling of latent(s) from training distribution.\"\"\"\n",
    "    return _sample_latents(latents, n_samples=n, expvar=expvar, pca=_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04da2fc",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDF\n",
    "from src.utils import make_grid2d, compute_sdf\n",
    "\n",
    "idx = 0\n",
    "clamp = True\n",
    "\n",
    "clamp &= clampD is not None and clampD > 0.\n",
    "latent = latents(torch.tensor([idx]).cuda()) if idx is not None else sample_latents()\n",
    "print(f\"Clamping at {clampD}.\" if clamp else \"No clamping.\")\n",
    "\n",
    "model.eval()\n",
    "cmap = colormaps['bwr']\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 3.5))\n",
    "for i, (ax, ax_name) in enumerate(zip(axs.flatten(), ['x', 'y', 'z'])):\n",
    "    xyz = make_grid2d([[-1, -1], [1, 1]], 512, i, 0.)\n",
    "    with torch.no_grad():\n",
    "        sdf = compute_sdf(model, latent, xyz.cuda()).squeeze().detach().cpu()\n",
    "\n",
    "    vmax = min(vmax, clampD) if clamp else sdf.abs().max()\n",
    "    ax.set_title(f\"SDF at {ax_name}=0.\")\n",
    "    im = ax.imshow(sdf.T.flip(0), cmap=cmap, vmin=-vmax, vmax=vmax, extent=[-1,1,-1,1])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5205ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesh\n",
    "from src.mesh import create_mesh\n",
    "\n",
    "idx = 0\n",
    "latent = latents(torch.tensor([idx]).cuda()) if idx is not None else sample_latents()\n",
    "mesh = create_mesh(model, latent, 256, 32**3, verbose=True)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f181e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering\n",
    "from src import visualization as viz\n",
    "\n",
    "image = viz.render_mesh(mesh)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd792b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "from src.reconstruct import reconstruct\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with open(specs[\"TestSplit\"]) as f:\n",
    "    instances_t = json.load(f)\n",
    "instance = instances_t[idx]\n",
    "print(f\"Reconstructing test shape {idx} ({instance})\")\n",
    "\n",
    "filename = os.path.join(specs[\"DataSource\"], specs[\"SamplesDir\"], instance, specs[\"SamplesFile\"])\n",
    "npz = np.load(filename)\n",
    "\n",
    "err, latent = reconstruct(model, npz, 400, 8000, 5e-3, loss_recon, latent_reg, clampD, None, latent_dim, verbose=True)\n",
    "print(f\"Final error: {err:.6f}.\")\n",
    "print(f\"Latent: norm={latent.norm():.4f} - std={latent.std():.4f}\")\n",
    "test_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=True, verbose=True)\n",
    "\n",
    "filename = os.path.join(specs[\"DataSource\"], \"meshes\", instance+\".obj\")\n",
    "gt_mesh = trimesh.load(filename)\n",
    "\n",
    "images = viz.render_meshes([gt_mesh, test_mesh])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(images[0]); axs[0].set_title(\"GT\")\n",
    "axs[1].imshow(images[1]); axs[1].set_title(\"Reconstruction\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from src.metric import chamfer_distance\n",
    "\n",
    "chamfer_samples = 30_000\n",
    "\n",
    "gt_samples = gt_mesh.sample(chamfer_samples)\n",
    "recon_samples = test_mesh.sample(chamfer_samples)\n",
    "chamfer_val = chamfer_distance(gt_samples, recon_samples)\n",
    "print(f\"Chamfer-distance = {chamfer_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914756c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f22d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Stop here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7151678",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f19bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
