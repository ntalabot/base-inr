{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f693af",
   "metadata": {},
   "source": [
    "Visualize results and trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c562136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e986e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from src import visualization as viz\n",
    "from src import workspace as ws\n",
    "from src.loss import get_loss_recon\n",
    "from src.mesh import create_mesh\n",
    "from src.metric import chamfer_distance\n",
    "from src.reconstruct import reconstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256378d",
   "metadata": {},
   "source": [
    "# Single experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f23bd",
   "metadata": {},
   "source": [
    "## Load exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d573c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed\n",
    "\n",
    "seed = 0\n",
    "expdir = \"../experiments/src_test/\"\n",
    "specs = ws.load_specs(expdir)\n",
    "\n",
    "print(f\"Experiment {expdir}\")\n",
    "#set_seed(seed); print(f\"Seeds initialized to {seed}.\")\n",
    "\n",
    "clampD = specs[\"ClampingDistance\"]\n",
    "latent_reg = specs[\"LatentRegLambda\"]\n",
    "\n",
    "logs = ws.load_history(expdir)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(13,4))\n",
    "\n",
    "for i, name in enumerate(['loss', 'loss_reg', 'lr', 'lat_norm']):\n",
    "    axs[i].set_title(name)\n",
    "    axs[i].plot(range(logs['epoch']), logs[name])\n",
    "    if name+\"_val\" in logs:\n",
    "        axs[i].plot(range(logs['epoch']), logs[name+\"_val\"])\n",
    "        axs[i].legend(['train', 'valid'])\n",
    "axs[2].plot(range(logs['epoch']), logs['lr_lat'])\n",
    "axs[2].legend(['lr', 'lr_lat'])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6c2b5",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = specs[\"SamplesPerScene\"]\n",
    "\n",
    "with open(specs[\"TrainSplit\"]) as f:\n",
    "    instances = json.load(f)\n",
    "if specs.get(\"ValidSplit\", None) is not None:\n",
    "    with open(specs[\"ValidSplit\"]) as f:\n",
    "        instances_v = json.load(f)\n",
    "else:\n",
    "    instances_v = []\n",
    "if specs.get(\"TestSplit\", None) is not None:\n",
    "    with open(specs[\"TestSplit\"]) as f:\n",
    "        instances_t = json.load(f)\n",
    "else:\n",
    "    instances_t = []\n",
    "\n",
    "print(f\"{len(instances)} shapes in train dataset.\")\n",
    "print(f\"{len(instances_v)} shapes in valid dataset.\")\n",
    "print(f\"{len(instances_t)} shapes in test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082042c",
   "metadata": {},
   "source": [
    "## Model and latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import get_model, get_latents\n",
    "\n",
    "cp_epoch = logs['epoch']\n",
    "latent_dim = specs['LatentDim']\n",
    "model = get_model(specs[\"Network\"], **specs.get(\"NetworkSpecs\", {}), latent_dim=latent_dim).cuda()\n",
    "latents = get_latents(len(instances), latent_dim, specs.get(\"LatentBound\", None))\n",
    "\n",
    "try:\n",
    "    ws.load_model(expdir, model, cp_epoch)\n",
    "    ws.load_latents(expdir, latents, cp_epoch)\n",
    "    print(f\"Loaded checkpoint of epoch={cp_epoch}\")\n",
    "except FileNotFoundError as err:\n",
    "    checkpoint = ws.load_checkpoint(expdir)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    latents.load_state_dict(checkpoint['latents_state_dict'])\n",
    "    print(f\"File not found: {err.filename}.\\nLoading checkpoint instead (epoch={checkpoint['epoch']}).\")\n",
    "    del checkpoint\n",
    "\n",
    "# Freeze to avoid possible gradient computations\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "if False:\n",
    "    print(\"Model:\", model)\n",
    "print(f\"Model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")\n",
    "print(f\"{latents.num_embeddings} latent vectors of size {latents.embedding_dim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6186b95",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04463c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.utils import sample_latents as _sample_latents\n",
    "\n",
    "_pca = PCA(whiten=True).fit(latents.weight.detach().cpu().numpy())\n",
    "def sample_latents(n=1, expvar=None):\n",
    "    \"\"\"PCA sampling of latent(s) from training distribution.\"\"\"\n",
    "    return _sample_latents(latents, n_samples=n, expvar=expvar, pca=_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd635f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(instances))\n",
    "cp_epoch = None\n",
    "print(f\"Shape {idx}: {instances[idx]}\")\n",
    "if cp_epoch is not None:\n",
    "    ws.load_model(expdir, model, cp_epoch)\n",
    "    ws.load_latents(expdir, latents, cp_epoch)\n",
    "    print(f\"Loaded checkpoint of epoch={cp_epoch}\")\n",
    "latent = latents(torch.tensor([idx]).cuda())\n",
    "\n",
    "train_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=True, verbose=True)\n",
    "gt_mesh = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx]+\".obj\"))\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "train_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95c373",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b68b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(instances), size=2).tolist()\n",
    "t = 0.5\n",
    "cp_epoch = None\n",
    "print(f\"Shapes {idx}: {instances[idx[0]]}, {instances[idx[1]]} (t={t:.2f})\")\n",
    "if cp_epoch is not None:\n",
    "    ws.load_model(expdir, model, cp_epoch)\n",
    "    ws.load_latents(expdir, latents, cp_epoch)\n",
    "    print(f\"Loaded checkpoint of epoch={cp_epoch}\")\n",
    "latent = latents(torch.tensor(idx).cuda())\n",
    "latent = (1. - t) * latent[0] + t * latent[1]\n",
    "\n",
    "interp_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=True, verbose=True)\n",
    "gt_mesh0 = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx[0]]+\".obj\"))\n",
    "gt_mesh1 = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instances[idx[1]]+\".obj\"))\n",
    "viz.plot_render(\n",
    "    [gt_mesh0, interp_mesh, gt_mesh1],\n",
    "    titles=[\"GT 0\", f\"Reconstruction (t={t:.2f})\", \"GT 1\"]\n",
    ").show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "interp_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387328b",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_epoch = None\n",
    "if cp_epoch is not None:\n",
    "    ws.load_model(expdir, model, cp_epoch)\n",
    "    print(f\"Loaded checkpoint of epoch={cp_epoch}\")\n",
    "latent = sample_latents()\n",
    "print(f\"Latent norm = {latent.norm().item():.6f}\")\n",
    "\n",
    "rand_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=True, verbose=True)\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "rand_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa495e10",
   "metadata": {},
   "source": [
    "### Valid/Test\n",
    "First, try to load an already reconstructed shape. If not, will optimize a latent and save the results (without overwriting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "_instances = instances_t  # valid or test\n",
    "always_reconstruct = False  # True to force reconstruction (do not overwrite existing files)\n",
    "idx = np.random.choice(len(_instances))\n",
    "instance = _instances[idx]\n",
    "print(f\"Reconstructing test shape {idx} ({instance})\")\n",
    "\n",
    "latent_subdir = ws.get_recon_latent_subdir(expdir, cp_epoch)\n",
    "mesh_subdir = ws.get_recon_mesh_subdir(expdir, cp_epoch)\n",
    "os.makedirs(latent_subdir, exist_ok=True)\n",
    "os.makedirs(mesh_subdir, exist_ok=True)\n",
    "latent_fn = os.path.join(latent_subdir, instance + \".pth\")\n",
    "mesh_fn = os.path.join(mesh_subdir, instance + \".obj\")\n",
    "\n",
    "loss_recon = get_loss_recon(\"L1-Hard\", reduction='none')\n",
    "\n",
    "# Latent: load existing or reconstruct\n",
    "if not always_reconstruct and os.path.isfile(latent_fn):\n",
    "    latent = torch.load(latent_fn)\n",
    "    print(f\"Latent norm = {latent.norm():.4f} (existing)\")\n",
    "else:\n",
    "    npz = np.load(os.path.join(specs[\"DataSource\"], specs[\"SamplesDir\"], instance, specs[\"SamplesFile\"]))\n",
    "    err, latent = reconstruct(model, npz, 400, 8000, 5e-3, loss_recon, latent_reg, clampD, None, latent_dim, verbose=True)\n",
    "    print(f\"Final loss: {err:.6f}, Latent norm = {latent.norm():.4f}\")\n",
    "    if not os.path.isfile(latent_fn):  # save reconstruction\n",
    "        torch.save(latent, latent_fn)\n",
    "# Mesh: load existing or reconstruct\n",
    "if not always_reconstruct and os.path.isfile(mesh_fn):\n",
    "    test_mesh = trimesh.load(mesh_fn)\n",
    "else:\n",
    "    test_mesh = create_mesh(model, latent, 256, 32**3, grid_filler=True, verbose=True)\n",
    "    if not os.path.isfile(mesh_fn):  # save reconstruction\n",
    "        test_mesh.export(mesh_fn)\n",
    "gt_mesh = trimesh.load(os.path.join(specs[\"DataSource\"], \"meshes\", instance+\".obj\"))\n",
    "\n",
    "# Chamfer\n",
    "chamfer_samples = 30_000\n",
    "chamfer_val = chamfer_distance(gt_mesh.sample(chamfer_samples), test_mesh.sample(chamfer_samples))\n",
    "print(f\"Chamfer-distance (x10^4) = {chamfer_val * 1e4:.6f}\")\n",
    "\n",
    "viz.plot_render([gt_mesh, test_mesh], titles=[\"GT\", \"Reconstruction\"]).show()\n",
    "viz.plot_sdf_slices(model, latent, clampD=clampD, contour=False).show()\n",
    "\n",
    "test_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408349b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187aa33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop here in case \"Run All\" has been used.\n",
    "raise RuntimeError(\"Stop here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4012f",
   "metadata": {},
   "source": [
    "# Misc.\n",
    "Misc. code to test various things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eeb3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
